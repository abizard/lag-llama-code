{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradien terhadap prediksi: -1.0466697216033936\n",
      "Prediksi setelah pembaruan: 2.721996784210205\n",
      "Nilai NLL: -2.4837968349456787\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Nilai aktual\n",
    "y_actual = torch.tensor([-0.006289])  # nilai aktual\n",
    "\n",
    "# Prediksi dari model\n",
    "y_pred = torch.tensor([2.71153], requires_grad=True)  # prediksi, dengan gradien diizinkan\n",
    "\n",
    "# Derajat kebebasan\n",
    "nu = 3.0  # nilai tetap\n",
    "\n",
    "# Parameter normalisasi\n",
    "Z = 0.0  # misalnya\n",
    "\n",
    "# Hitung selisih\n",
    "y_diff = y_actual - y_pred  # menghitung selisih antara nilai aktual dan prediksi\n",
    "\n",
    "# Hitung NLL\n",
    "nll = -0.5 * (nu + 1) * torch.log1p((y_diff ** 2) / nu) - Z  # Menggunakan log1p untuk stabilitas\n",
    "\n",
    "# Hitung propagasi mundur\n",
    "nll.backward()  # menghitung gradien dari NLL terhadap semua parameter yang memiliki requires_grad=True\n",
    "\n",
    "# Ambil gradien dari prediksi\n",
    "grad_y_pred = y_pred.grad  # mendapatkan gradien terhadap prediksi\n",
    "\n",
    "# Tentukan learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Perbarui parameter prediksi\n",
    "y_pred.data -= learning_rate * grad_y_pred  # memperbarui nilai prediksi berdasarkan gradien\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"Gradien terhadap prediksi:\", grad_y_pred.item())\n",
    "print(\"Prediksi setelah pembaruan:\", y_pred.data.item())\n",
    "print(\"Nilai NLL:\", nll.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0171], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentT(df: 3.0, loc: 2.7115299701690674, scale: 1.452583909034729)\n",
      "tensor([2.9208], grad_fn=<NegBackward0>)\n",
      "2.920837163925171\n",
      "NLL untuk baris 1: 2.920837\n",
      "Total NLL: 2.920837\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Data\n",
    "# y_actual = torch.tensor([0.7738])\n",
    "# y_pred = torch.tensor([0.8000, 0.5400, 0.5500, 0.6800])\n",
    "y_actual = torch.tensor([-0.006289])  # nilai aktual\n",
    "\n",
    "# Prediksi dari model\n",
    "y_pred = torch.tensor([2.71153], requires_grad=True) \n",
    "print(y_actual*y_pred)\n",
    "variance = torch.tensor([2.11])\n",
    "beta = 0.0\n",
    "\n",
    "# Distribusi Normal untuk setiap prediksi\n",
    "nll_losses = []\n",
    "for i in range(len(y_actual)):\n",
    "    studentt = dist.StudentT(loc=y_pred[i], scale=torch.sqrt(variance[i]), df=3)\n",
    "    print(studentt)\n",
    "    nll = -studentt.log_prob(y_actual)\n",
    "    print(nll)\n",
    "    \n",
    "    # Jika β-weighting digunakan\n",
    "    if beta > 0:\n",
    "        nll = nll * (variance[i] ** beta)\n",
    "    print(nll.item())\n",
    "    nll_losses.append(nll.item())\n",
    "\n",
    "# Print hasil NLL untuk setiap baris\n",
    "for i, nll in enumerate(nll_losses):\n",
    "    print(f\"NLL untuk baris {i+1}: {nll:.6f}\")\n",
    "\n",
    "# Total NLL (sum atau average bisa dipilih)\n",
    "total_nll = sum(nll_losses)\n",
    "print(f\"Total NLL: {total_nll:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0016,  0.0077,  0.0106,  0.0154,  0.0085,  0.0107, -0.0001, -0.0034])\n",
      "tensor([-0.0035, -0.0063])\n",
      "tensor([[ 0.0016,  0.0077,  0.0106,  0.0154,  0.0085,  0.0107, -0.0001, -0.0034]])\n",
      "Prediksi x₉: 0.326751\n",
      "tensor([[ 7.6520e-03,  1.0573e-02,  1.5362e-02,  8.5020e-03,  1.0746e-02,\n",
      "         -1.3500e-04, -3.3590e-03,  3.2675e-01]], grad_fn=<UnsqueezeBackward0>)\n",
      "Prediksi x₁₀: 0.741542\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Data Input (8 data pertama) dan Target (2 data terakhir)\n",
    "data = torch.tensor([\n",
    "    0.00165, 0.007652, 0.010573, 0.015362, \n",
    "    0.008502, 0.010746, -0.000135, -0.003359, \n",
    "    -0.003548, -0.006289\n",
    "], dtype=torch.float32)\n",
    "\n",
    "X = data[:8]  # Input\n",
    "print(X)\n",
    "Y_actual = data[8:]  # Target\n",
    "print(Y_actual)\n",
    "\n",
    "# Simple Decoder-Only Model (Lag-Llama Style)\n",
    "class LagLlama(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LagLlama, self).__init__()\n",
    "        self.rmsnorm = nn.LayerNorm(input_size)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_size, num_heads=1)\n",
    "        self.fc = nn.Linear(input_size, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rmsnorm(x)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n",
    "\n",
    "# Inisialisasi Model\n",
    "model = LagLlama(input_size=8)\n",
    "\n",
    "# Prediksi Pertama (x₉)\n",
    "X_input = X.unsqueeze(0)  # Tambahkan batch dimension\n",
    "print(X_input)\n",
    "pred_9 = model(X_input)\n",
    "pred_9 = pred_9.squeeze(-1)  # Hilangkan dimensi ekstra jika ada\n",
    "print(f\"Prediksi x₉: {pred_9.item():.6f}\")\n",
    "\n",
    "# Prediksi Kedua (x₁₀)\n",
    "# Pastikan pred_9 memiliki dimensi yang konsisten dengan X[1:]\n",
    "X_input_next = torch.cat([X[1:], pred_9.view(1)], dim=0).unsqueeze(0)  # Shift input\n",
    "print(X_input_next)\n",
    "\n",
    "pred_10 = model(X_input_next)\n",
    "print(f\"Prediksi x₁₀: {pred_10.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=4, bias=True)\n",
      "Embedded Tokens:\n",
      " tensor([[ 0.4167,  0.0723,  0.3051, -0.2716],\n",
      "        [ 0.3404,  0.1797,  0.1877, -0.1961],\n",
      "        [ 0.3814, -0.2512,  0.1169,  0.1060],\n",
      "        [ 0.3181,  0.0702,  0.4226, -0.1643],\n",
      "        [ 0.4571, -0.4543,  0.4006,  0.0442],\n",
      "        [ 0.4488, -0.2273,  0.9171, -0.3145],\n",
      "        [ 0.5233, -0.1446,  0.9295, -0.4971]], grad_fn=<AddmmBackward0>)\n",
      "Shape: torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data token (7 timesteps, 4 fitur per timestep)\n",
    "tokens = torch.tensor([[ 0.4697,  0.2380, -0.2380],\n",
    "        [ 0.8495,  0.4697,  0.2380],\n",
    "        [ 0.3054,  0.8495,  0.4697],\n",
    "        [ 0.4834,  0.3054,  0.8495],\n",
    "        [-0.3796,  0.4834,  0.3054],\n",
    "        [-0.6353, -0.3796,  0.4834],\n",
    "        [-0.6503, -0.6353, -0.3796]], dtype=torch.float32)  # Shape: (7, 4)\n",
    "\n",
    "# Definisikan layer embedding linear\n",
    "embedding_layer = nn.Linear(3, 4)  # Proyeksi dari 4 fitur ke 8 dimensi\n",
    "print(embedding_layer)\n",
    "\n",
    "# Terapkan embedding\n",
    "embedded_tokens = embedding_layer(tokens)\n",
    "print(\"Embedded Tokens:\\n\", embedded_tokens)\n",
    "print(\"Shape:\", embedded_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSNorm()\n",
      "Normalized Tokens:\n",
      " tensor([[ 1.4173,  0.2459,  1.0378, -0.9238],\n",
      "        [ 1.4454,  0.7628,  0.7970, -0.8325],\n",
      "        [ 1.5784, -1.0398,  0.4838,  0.4388],\n",
      "        [ 1.1395,  0.2514,  1.5137, -0.5886],\n",
      "        [ 1.2026, -1.1954,  1.0540,  0.1163],\n",
      "        [ 0.8217, -0.4162,  1.6792, -0.5759],\n",
      "        [ 0.8828, -0.2439,  1.5679, -0.8384]], grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, size: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))  # Parameter skala (trainable)\n",
    "        self.eps = eps  # Untuk mencegah pembagian nol\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Hitung RMS (Root Mean Square)\n",
    "        norm_x = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        rms_x = torch.sqrt(norm_x + self.eps)\n",
    "        \n",
    "        # Normalisasi dan terapkan skala trainable\n",
    "        x_normed = x / rms_x\n",
    "        return self.scale * x_normed\n",
    "\n",
    "# Contoh RMSNorm untuk embedded tokens\n",
    "rmsnorm = RMSNorm(size=4)\n",
    "print(rmsnorm)\n",
    "\n",
    "normalized_tokens = rmsnorm(embedded_tokens)\n",
    "print(\"Normalized Tokens:\\n\", normalized_tokens)\n",
    "print(\"Shape:\", normalized_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKVProjection(\n",
      "  (q_proj): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (k_proj): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (v_proj): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Matriks Q:\n",
      " tensor([[ 0.8554, -0.0846,  0.0809,  0.3194],\n",
      "        [ 1.0166,  0.1040,  0.2562,  0.2669],\n",
      "        [ 0.6709,  0.5092,  0.2853,  0.9109],\n",
      "        [ 0.9918, -0.2330, -0.1799,  0.5199],\n",
      "        [ 0.5214,  0.0289, -0.1140,  0.8195],\n",
      "        [ 0.6369, -0.4936, -0.4433,  0.5148],\n",
      "        [ 0.6128, -0.5068, -0.3746,  0.3720]], grad_fn=<MmBackward0>)\n",
      "Matriks K:\n",
      " tensor([[-0.1401,  0.4000,  0.1707,  0.1956],\n",
      "        [-0.2047,  0.4241, -0.1946,  0.2855],\n",
      "        [ 0.6553,  0.3508, -0.1717, -0.6181],\n",
      "        [-0.2861,  0.2350,  0.2838,  0.4305],\n",
      "        [ 0.3966,  0.2196,  0.3934, -0.3465],\n",
      "        [-0.2144,  0.1241,  0.7472,  0.3202],\n",
      "        [-0.2629,  0.1821,  0.7336,  0.3420]], grad_fn=<MmBackward0>)\n",
      "Matriks V:\n",
      " tensor([[ 0.7864,  0.8289, -0.6233, -0.5611],\n",
      "        [ 0.8741,  0.6864, -0.8102, -0.7721],\n",
      "        [-0.6127,  0.8568, -0.5466, -0.5855],\n",
      "        [ 0.8691,  0.8826, -0.5689, -0.7003],\n",
      "        [-0.2697,  0.9396, -0.2680, -0.3409],\n",
      "        [ 0.6690,  0.9164, -0.2106, -0.3268],\n",
      "        [ 0.8060,  0.8815, -0.2401, -0.2815]], grad_fn=<MmBackward0>)\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "class QKVProjection(nn.Module):\n",
    "    def __init__(self, emb_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q_proj(x)  # Proyeksi query\n",
    "        K = self.k_proj(x)  # Proyeksi key\n",
    "        V = self.v_proj(x)  # Proyeksi value\n",
    "        return Q, K, V\n",
    "\n",
    "# Inisialisasi dengan ukuran embedding 8 dan head dimension 8\n",
    "qkv_layer = QKVProjection(emb_dim=4, head_dim=4)\n",
    "print(qkv_layer)\n",
    "\n",
    "Q, K, V = qkv_layer(normalized_tokens)\n",
    "\n",
    "print(\"Matriks Q:\\n\", Q)\n",
    "print(\"Matriks K:\\n\", K)\n",
    "print(\"Matriks V:\\n\", V)\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000],\n",
      "        [ 0.5403],\n",
      "        [-0.4161],\n",
      "        [-0.9900],\n",
      "        [-0.6536],\n",
      "        [ 0.2837],\n",
      "        [ 0.9602]])\n",
      "tensor([[ 0.0000],\n",
      "        [ 0.8415],\n",
      "        [ 0.9093],\n",
      "        [ 0.1411],\n",
      "        [-0.7568],\n",
      "        [-0.9589],\n",
      "        [-0.2794]])\n",
      "Q dengan RoPE:\n",
      " tensor([[ 0.8554, -0.0846,  0.0809,  0.3194],\n",
      "        [ 0.3337, -0.1684,  0.9939,  0.2317],\n",
      "        [-0.5386, -1.0402,  0.4913,  0.0840],\n",
      "        [-0.9565,  0.1573,  0.3181, -0.5476],\n",
      "        [-0.4271,  0.6013, -0.3201, -0.5575],\n",
      "        [-0.2445,  0.3536, -0.7365,  0.6193],\n",
      "        [ 0.4837, -0.3827, -0.5309,  0.4988]], grad_fn=<AddBackward0>)\n",
      "K dengan RoPE:\n",
      " tensor([[-0.1401,  0.4000,  0.1707,  0.1956],\n",
      "        [ 0.0532, -0.0111, -0.2774,  0.5112],\n",
      "        [-0.1166,  0.4160,  0.6673,  0.5762],\n",
      "        [ 0.2432, -0.2934, -0.3213, -0.3930],\n",
      "        [ 0.0385, -0.4058, -0.5573,  0.0603],\n",
      "        [ 0.6557,  0.3423,  0.4175, -0.0281],\n",
      "        [-0.0474,  0.2704,  0.7779,  0.2775]], grad_fn=<AddBackward0>)\n",
      "torch.Size([7, 4]) torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Pisahkan separuh dimensi dan rotasi bagian kedua.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rope(q, k, cos, sin, position_ids):\n",
    "    \"\"\"Terapkan RoPE dengan cosine dan sine pada Q dan K.\"\"\"\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)\n",
    "    print(cos)\n",
    "    print(sin)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# Terapkan RoPE pada Q dan K\n",
    "# Contoh cosine dan sine\n",
    "cos = torch.cos(torch.arange(0, 10).unsqueeze(1))  # Simulasi cosine untuk 10 posisi\n",
    "sin = torch.sin(torch.arange(0, 10).unsqueeze(1))  # Simulasi sine untuk 10 posisi\n",
    "\n",
    "# Terapkan RoPE pada Q dan K\n",
    "position_ids = torch.arange(Q.shape[0])  # [0, 1, 2, ..., 9]\n",
    "Q_rope, K_rope = apply_rope(Q, K, cos, sin, position_ids)\n",
    "\n",
    "print(\"Q dengan RoPE:\\n\", Q_rope)\n",
    "print(\"K dengan RoPE:\\n\", K_rope)\n",
    "print(Q_rope.shape, K_rope.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "Attention Output:\n",
      " tensor([[[[ 1.2906, -0.0791, -0.3988,  1.7156],\n",
      "          [ 0.6659,  0.2351, -0.2672,  0.2181],\n",
      "          [-0.1676,  0.0192, -0.0077,  0.4206],\n",
      "          [-0.7717,  0.1047,  0.8664,  0.0109],\n",
      "          [-0.4598,  0.4282,  1.5111, -0.0337],\n",
      "          [-0.1040,  0.4174,  0.7301, -0.1205],\n",
      "          [-0.6456,  0.5434,  1.3861, -0.1357]]]])\n",
      "Shape: torch.Size([1, 1, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "def masked_causal_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Implementasi Masked Causal Self-Attention.\n",
    "    \"\"\"\n",
    "    # Ukuran batch dan sequence length\n",
    "    batch_size, seq_len, head_dim = Q.size()\n",
    "\n",
    "    # Langkah 1: Hitung skor perhatian (Q @ K^T)\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "\n",
    "    # Langkah 2: Terapkan masking (optional)\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Langkah 3: Softmax untuk normalisasi skor\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # Langkah 4: Kalikan dengan V untuk mendapatkan output\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return attention_output\n",
    "\n",
    "# Contoh input: Matriks Q, K, dan V\n",
    "batch_size, seq_len, head_dim = 1, 7, 4  # Satu batch, 7 timestep, 8 dimensi per token\n",
    "\n",
    "Q_rope = torch.randn(batch_size, seq_len, head_dim)  # Query dengan RoPE\n",
    "K_rope = torch.randn(batch_size, seq_len, head_dim)  # Key dengan RoPE\n",
    "V = torch.randn(batch_size, seq_len, head_dim)       # Value\n",
    "\n",
    "# Buat mask segitiga bawah untuk causal masking\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)  # (1, 1, 7, 7)\n",
    "print(mask)\n",
    "# Hitung attention output\n",
    "attention_output = masked_causal_attention(Q_rope, K_rope, V, mask=mask)\n",
    "\n",
    "print(\"Attention Output:\\n\", attention_output)\n",
    "print(\"Shape:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Output:\n",
      " tensor([[[[ 1.5257, -0.0061, -0.0837,  1.2904],\n",
      "          [ 1.8438,  0.7600, -0.1457,  0.0403],\n",
      "          [ 0.6855, -0.7443,  0.3501,  1.6891],\n",
      "          [-0.6544,  0.2523,  1.8598, -0.2213],\n",
      "          [-0.0029, -0.0273,  1.9998,  0.0110],\n",
      "          [ 0.3944,  0.2174,  1.8840, -0.4976],\n",
      "          [-0.1004,  0.3274,  1.9008, -0.5194]]]], grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([1, 1, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.rms_norm = RMSNorm(size=emb_dim)  # RMSNorm setelah residual connection\n",
    "\n",
    "    def forward(self, input_tokens, attention_output):\n",
    "        # Residual connection: Tambahkan input ke attention output\n",
    "        residual_output = input_tokens + attention_output\n",
    "\n",
    "        # Terapkan RMSNorm\n",
    "        normalized_output = self.rms_norm(residual_output)\n",
    "        return normalized_output\n",
    "\n",
    "# Inisialisasi residual layer\n",
    "residual_layer = ResidualLayer(emb_dim=4)  # Dimensi embedding = 8\n",
    "\n",
    "# Input awal (embedded tokens) dan attention output\n",
    "# embedded_tokens = torch.randn(1, 7, 4)  # Contoh embedded tokens (batch=1, timesteps=7, emb_dim=8)\n",
    "# attention_output = torch.randn(1, 7, 4)  # Output dari masked causal self-attention\n",
    "\n",
    "# Terapkan residual connection dan RMSNorm\n",
    "residual_output = residual_layer(embedded_tokens, attention_output)\n",
    "\n",
    "print(\"Residual Output:\\n\", residual_output)\n",
    "print(\"Shape:\", residual_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Output (SwiGLU):\n",
      " tensor([[[ 0.4490, -0.4811,  0.8380, -0.4820],\n",
      "         [ 0.5467, -0.3862,  0.9281, -0.6283],\n",
      "         [ 0.5068, -0.4250,  0.8913, -0.5685],\n",
      "         [ 0.3958, -0.5327,  0.7891, -0.4026],\n",
      "         [ 0.5174, -0.4147,  0.9011, -0.5844],\n",
      "         [ 0.5551, -0.3781,  0.9358, -0.6408],\n",
      "         [ 0.4588, -0.4716,  0.8471, -0.4967]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([1, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Dua layer linear untuk proyeksi\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)  # W1 dan b1\n",
    "        self.fc2 = nn.Linear(emb_dim, hidden_dim)  # W2 dan b2\n",
    "        self.fc_out = nn.Linear(hidden_dim, emb_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Proyeksi pertama dengan SiLU\n",
    "        x1 = F.silu(self.fc1(x))  # SiLU(W1 * x + b1)\n",
    "        # Proyeksi kedua linear\n",
    "        x2 = self.fc2(x)  # W2 * x + b2\n",
    "\n",
    "        # Elemen-wise multiplication\n",
    "        out = x1 * x2  # SwiGLU\n",
    "        return self.fc_out(out)  # Output layer\n",
    "\n",
    "# Inisialisasi layer SwiGLU\n",
    "swiglu_layer = SwiGLU(emb_dim=4, hidden_dim=1)  # Embedding dim 8, hidden dim 32\n",
    "\n",
    "# Contoh input: Output dari residual connection sebelumnya\n",
    "# residual_output = torch.randn(1, 7, 4)  # Batch=1, Sequence Length=7, Embedding Dim=8\n",
    "\n",
    "# Terapkan SwiGLU\n",
    "feed_forward_output = swiglu_layer(residual_output)\n",
    "\n",
    "print(\"Feed Forward Output (SwiGLU):\\n\", feed_forward_output)\n",
    "print(\"Shape:\", feed_forward_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output:\n",
      " tensor([[[ 1.4249,  0.2248,  1.3836,  0.0692],\n",
      "         [ 0.0111, -0.8981,  0.5018, -1.7151],\n",
      "         [ 1.8350, -0.5532, -0.5143, -0.2494],\n",
      "         [ 0.1115, -0.7550, -0.0445, -1.8481],\n",
      "         [-0.0155,  0.5651,  1.7255,  0.8385],\n",
      "         [ 0.4516, -1.2546,  0.6572, -1.3379],\n",
      "         [-0.3666, -1.6895,  0.8242, -0.5762]]], grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([1, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, size: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))  # Skala trainable\n",
    "        self.eps = eps  # Untuk mencegah pembagian nol\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Hitung RMS (Root Mean Square)\n",
    "        norm_x = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        rms_x = torch.sqrt(norm_x + self.eps)\n",
    "\n",
    "        # Normalisasi dan skala\n",
    "        x_normed = x / rms_x\n",
    "        return self.scale * x_normed\n",
    "\n",
    "class ResidualWithRMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.rms_norm = RMSNorm(size=emb_dim)\n",
    "\n",
    "    def forward(self, input_tokens, feed_forward_output):\n",
    "        # Residual Connection\n",
    "        residual_output = input_tokens + feed_forward_output\n",
    "\n",
    "        # RMSNorm\n",
    "        normalized_output = self.rms_norm(residual_output)\n",
    "        return normalized_output\n",
    "\n",
    "# Inisialisasi residual layer dengan RMSNorm\n",
    "residual_with_rmsnorm_layer = ResidualWithRMSNorm(emb_dim=4)\n",
    "\n",
    "# Input: Output dari residual sebelumnya dan feed-forward output\n",
    "# residual_input = torch.randn(1, 7, 4)  # (Batch=1, Seq Len=7, Emb Dim=8)\n",
    "# feed_forward_output = torch.randn(1, 7, 4)  # Hasil dari SwiGLU\n",
    "\n",
    "# Terapkan residual connection dan RMSNorm\n",
    "final_output = residual_with_rmsnorm_layer(residual_input, feed_forward_output)\n",
    "\n",
    "print(\"Final Output:\\n\", final_output)\n",
    "print(\"Shape:\", final_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.view(-1,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.distributions import StudentTOutput\n",
    "\n",
    "class DistributionHead(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.param_proj = nn.Linear(emb_dim, 3)\n",
    "        self.distr_output = StudentTOutput()\n",
    "\n",
    "    def forward(self, x):\n",
    "        distr_params = self.param_proj(x)\n",
    "\n",
    "        df = F.softplus(distr_params[..., 0]) + 1e-5\n",
    "        loc = distr_params[..., 1]\n",
    "        scale = F.softplus(distr_params[..., 2])\n",
    "\n",
    "        distr = self.distr_output.distribution(df=df, loc=loc, scale=scale)\n",
    "\n",
    "        return distr, loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_head = DistributionHead(emb_dim=4)\n",
    "final_output = final_output\n",
    "\n",
    "distr, loc, scale = distribution_head(final_output.view(-1,4))\n",
    "predictions = distr.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KITA PERBARUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date    value\n",
      "0 2013-02-06 -0.23802\n",
      "1 2013-02-07  0.23802\n",
      "2 2013-02-08  0.46969\n",
      "3 2013-02-09  0.84952\n",
      "4 2013-02-10  0.30543\n",
      "5 2013-02-11  0.48341\n",
      "6 2013-02-12 -0.37959\n",
      "7 2013-02-13 -0.63530\n",
      "8 2013-02-14 -0.65029\n",
      "9 2013-02-15 -0.86769\n",
      "        date    value    lag_1    lag_2    lag_3\n",
      "3 2013-02-09  0.84952  0.46969  0.23802 -0.23802\n",
      "4 2013-02-10  0.30543  0.84952  0.46969  0.23802\n",
      "5 2013-02-11  0.48341  0.30543  0.84952  0.46969\n",
      "6 2013-02-12 -0.37959  0.48341  0.30543  0.84952\n",
      "7 2013-02-13 -0.63530 -0.37959  0.48341  0.30543\n",
      "8 2013-02-14 -0.65029 -0.63530 -0.37959  0.48341\n",
      "9 2013-02-15 -0.86769 -0.65029 -0.63530 -0.37959\n",
      "X_tensor: tensor([[ 0.4697,  0.2380, -0.2380],\n",
      "        [ 0.8495,  0.4697,  0.2380],\n",
      "        [ 0.3054,  0.8495,  0.4697],\n",
      "        [ 0.4834,  0.3054,  0.8495],\n",
      "        [-0.3796,  0.4834,  0.3054],\n",
      "        [-0.6353, -0.3796,  0.4834],\n",
      "        [-0.6503, -0.6353, -0.3796]])\n",
      "y_tensor: tensor([ 0.8495,  0.3054,  0.4834, -0.3796, -0.6353, -0.6503, -0.8677])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Contoh data time series\n",
    "data = {\n",
    "    'date': pd.date_range(start='2013-02-06', periods=10, freq='D'),\n",
    "    'value': [-0.23802, 0.23802, 0.46969, 0.84952, 0.30543, \n",
    "              0.48341, -0.37959, -0.6353, -0.65029, -0.86769]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Membuat lagged features\n",
    "def create_lagged_features(df, lag):\n",
    "    for i in range(1, lag + 1):\n",
    "        df[f'lag_{i}'] = df['value'].shift(i)\n",
    "    df = df.dropna()  # Hapus nilai NaN\n",
    "    return df\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Menambahkan 3 lag features\n",
    "df_lagged = create_lagged_features(df, lag=3)\n",
    "print(df_lagged)\n",
    "\n",
    "# Data fitur (X) dan target (y)\n",
    "X = df_lagged.drop(columns=['date', 'value']).values\n",
    "y = df_lagged['value'].values\n",
    "\n",
    "# Mengonversi ke tensor\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "print('X_tensor:', X_tensor)\n",
    "print('y_tensor:', y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MulheadAttention(embed_dim=embedding_dim, num_heads=4)\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)  # Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Feedforward\n",
    "        ff_output = self.fc(x)\n",
    "        x = x + self.dropout(ff_output)  # Residual connection\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Hasil Embedding:\n",
      "tensor([[-0.6979,  0.5118, -0.0960,  0.1352,  0.7720,  0.3822,  0.3115,  0.4134],\n",
      "        [-0.5246,  0.8096, -0.1240,  0.1079,  0.6297,  0.5786,  0.4924,  0.7130],\n",
      "        [-0.4350,  0.7897, -0.1868, -0.1055,  0.2342,  0.3228,  0.7014,  0.3502],\n",
      "        [-0.2262,  0.6745,  0.0572, -0.2512,  0.2114,  0.6158,  0.7440,  0.7986],\n",
      "        [-0.4469,  0.3568, -0.0412, -0.3047,  0.1281,  0.1122,  0.6536,  0.0475],\n",
      "        [-0.2809, -0.0806,  0.3098, -0.5506,  0.1044,  0.2718,  0.6142,  0.3015],\n",
      "        [-0.6369, -0.2960,  0.2602, -0.2688,  0.5860,  0.1362,  0.2395,  0.0709]])\n"
     ]
    }
   ],
   "source": [
    "class LLAMA(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, num_decoder_layers):\n",
    "        super(LLAMA, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, embedding_dim)  # Layer embedding\n",
    "        self.rmsnorm = nn.LayerNorm(embedding_dim)  # Normalisasi\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(embedding_dim) for _ in range(num_decoder_layers)])\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Layer output untuk prediksi\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Proses embedding\n",
    "        x = self.rmsnorm(x)    # Normalisasi\n",
    "        for decoder in self.decoder_blocks:\n",
    "            x = decoder(x)\n",
    "        x = self.fc(x)         # Layer output\n",
    "        return x\n",
    "\n",
    "# Parameter model\n",
    "input_size = X_tensor.shape[1]\n",
    "print(input_size)  # Jumlah fitur\n",
    "embedding_dim = 8             # Dimensi embedding\n",
    "model = LLAMA(input_size, embedding_dim, 3)\n",
    "\n",
    "def get_embeddings(model, data):\n",
    "    model.eval()  # Set model ke evaluasi\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embedding(data)  # Mendapatkan hasil embedding\n",
    "        return embeddings\n",
    "\n",
    "# Mendapatkan hasil embedding untuk data fitur\n",
    "embeddings = get_embeddings(model, X_tensor)\n",
    "\n",
    "# Menampilkan hasil embedding\n",
    "print(\"Hasil Embedding:\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-02-06', '2013-02-07', '2013-02-08', '2013-02-09', '2013-02-10', '2013-02-11', '2013-02-12', '2013-02-13', '2013-02-14', '2013-02-15']\n",
      "[0.00165, 0.007652, 0.010573, 0.015362, 0.008502, 0.010746, -0.000135, -0.003359, -0.003548, -0.006289]\n",
      "tensor([ 0.0016,  0.0077,  0.0106,  0.0154,  0.0085,  0.0107, -0.0001, -0.0034,\n",
      "        -0.0035, -0.0063])\n",
      "10 3\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "0 1\n",
      "tensor([[ 0.0106,  0.0000,  0.0000],\n",
      "        [ 0.0154,  0.0000,  0.0000],\n",
      "        [ 0.0085,  0.0000,  0.0000],\n",
      "        [ 0.0107,  0.0000,  0.0000],\n",
      "        [-0.0001,  0.0000,  0.0000],\n",
      "        [-0.0034,  0.0000,  0.0000],\n",
      "        [-0.0035,  0.0000,  0.0000]])\n",
      "1 2\n",
      "tensor([[ 0.0106,  0.0077,  0.0000],\n",
      "        [ 0.0154,  0.0106,  0.0000],\n",
      "        [ 0.0085,  0.0154,  0.0000],\n",
      "        [ 0.0107,  0.0085,  0.0000],\n",
      "        [-0.0001,  0.0107,  0.0000],\n",
      "        [-0.0034, -0.0001,  0.0000],\n",
      "        [-0.0035, -0.0034,  0.0000]])\n",
      "2 3\n",
      "tensor([[ 0.0106,  0.0077,  0.0016],\n",
      "        [ 0.0154,  0.0106,  0.0077],\n",
      "        [ 0.0085,  0.0154,  0.0106],\n",
      "        [ 0.0107,  0.0085,  0.0154],\n",
      "        [-0.0001,  0.0107,  0.0085],\n",
      "        [-0.0034, -0.0001,  0.0107],\n",
      "        [-0.0035, -0.0034, -0.0001]])\n",
      "Lag features (tokenized input):\n",
      "tensor([[ 0.0106,  0.0077,  0.0016],\n",
      "        [ 0.0154,  0.0106,  0.0077],\n",
      "        [ 0.0085,  0.0154,  0.0106],\n",
      "        [ 0.0107,  0.0085,  0.0154],\n",
      "        [-0.0001,  0.0107,  0.0085],\n",
      "        [-0.0034, -0.0001,  0.0107],\n",
      "        [-0.0035, -0.0034, -0.0001]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Data harga saham yang diberikan\n",
    "data = {\n",
    "    \"2013-02-06\": 0.00165,\n",
    "    \"2013-02-07\": 0.007652,\n",
    "    \"2013-02-08\": 0.010573,\n",
    "    \"2013-02-09\": 0.015362,\n",
    "    \"2013-02-10\": 0.008502,\n",
    "    \"2013-02-11\": 0.010746,\n",
    "    \"2013-02-12\": -0.000135,\n",
    "    \"2013-02-13\": -0.003359,\n",
    "    \"2013-02-14\": -0.003548,\n",
    "    \"2013-02-15\": -0.006289\n",
    "}\n",
    "\n",
    "# Mengkonversi data ke tensor PyTorch\n",
    "dates = list(data.keys())\n",
    "print(dates)\n",
    "values = list(data.values())\n",
    "print(values)\n",
    "values_tensor = torch.tensor(values, dtype=torch.float32)\n",
    "print(values_tensor)\n",
    "\n",
    "# Menentukan lag sequence\n",
    "lags_seq = [1, 2, 3]  # menggunakan lag 1, 2, dan 3 hari\n",
    "\n",
    "def lagged_sequence_values(lags_seq, values_tensor):\n",
    "    n = len(values_tensor)\n",
    "    max_lag = max(lags_seq)\n",
    "    print(n,max_lag)\n",
    "    # Inisialisasi tensor untuk menyimpan lagged features\n",
    "    lagged_features = torch.zeros((n-max_lag, len(lags_seq)))\n",
    "    print(lagged_features)\n",
    "    for idx, lag in enumerate(lags_seq):\n",
    "        print(idx,lag)\n",
    "        lagged_features[:, idx] = values_tensor[max_lag-lag:n-lag]\n",
    "        print(lagged_features)\n",
    "    return lagged_features\n",
    "\n",
    "# Mendapatkan fitur lag\n",
    "lag_features = lagged_sequence_values(lags_seq, values_tensor)\n",
    "\n",
    "print(\"Lag features (tokenized input):\")\n",
    "print(lag_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings,\n",
    "            device=self.inv_freq.device,\n",
    "            dtype=torch.get_default_dtype(),\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(\n",
    "            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype\n",
    "        )\n",
    "\n",
    "        freqs = torch.einsum(\"n,d->nd\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\n",
    "            \"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(self, device, dtype, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=device, dtype=dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=dtype),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagLlamaModelEmb(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim):\n",
    "        super(LagLlamaModelEmb, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, embedding_dim)  # Layer linear untuk embedding\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rotary_embedding = LlamaRotaryEmbedding(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Aplikasi embedding linear\n",
    "        cos, sin = self.rotary_embedding(x.device, x.dtype, x.shape[1])\n",
    "        return x, cos, sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0106,  0.0077,  0.0016],\n",
      "         [ 0.0154,  0.0106,  0.0077],\n",
      "         [ 0.0085,  0.0154,  0.0106],\n",
      "         [ 0.0107,  0.0085,  0.0154],\n",
      "         [-0.0001,  0.0107,  0.0085],\n",
      "         [-0.0034, -0.0001,  0.0107],\n",
      "         [-0.0035, -0.0034, -0.0001]]])\n",
      "Output of Rotary Embedding: tensor([[0.0716],\n",
      "        [0.0753],\n",
      "        [0.0750],\n",
      "        [0.0770],\n",
      "        [0.0718],\n",
      "        [0.0712],\n",
      "        [0.0668]], grad_fn=<AddmmBackward0>)\n",
      "Cosine Embedding: tensor([[[[1., 1.]]]])\n",
      "Sine Embedding: tensor([[[[0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Pengaturan model\n",
    "input_size = 3  # Misalkan input memiliki 3 fitur\n",
    "embedding_dim = 1  # Ukuran embedding yang diinginkan\n",
    "model = LagLlamaModelEmb(input_size, embedding_dim)\n",
    "\n",
    "# Input yang diberikan\n",
    "lag_features_new = lag_features.unsqueeze(0)\n",
    "print(lag_features_new)  # Menambahkan dimensi batch untuk konsistensi\n",
    "\n",
    "# Penggunaan model\n",
    "output, cos, sin = model(lag_features)\n",
    "print(\"Output of Rotary Embedding:\", output)\n",
    "print(\"Cosine Embedding:\", cos)\n",
    "print(\"Sine Embedding:\", sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSNorm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, size, eps=1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Menghitung mean square\n",
    "        mean_square = x.pow(2).mean(-1, keepdim=True)\n",
    "        # Normalisasi\n",
    "        normalized_x = x * torch.rsqrt(mean_square + self.eps)\n",
    "        return self.scale * normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagLlamaModelNorm1(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim):\n",
    "        super(LagLlamaModelNorm1, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, embedding_dim)  # Layer linear untuk embedding\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rotary_embedding = LlamaRotaryEmbedding(embedding_dim)\n",
    "        self.rms_norm = RMSNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Aplikasi embedding linear\n",
    "        x = self.rms_norm(x)\n",
    "        cos, sin = self.rotary_embedding(x.device, x.dtype, x.shape[1])\n",
    "        return x, cos, sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after RMSNorm: (tensor([[[-1.3549, -1.0243, -1.2932, -1.4913,  0.1329,  1.0596,  0.2657,\n",
      "           0.0835],\n",
      "         [-1.3541, -1.0176, -1.2958, -1.4855,  0.1396,  1.0681,  0.2737,\n",
      "           0.0951],\n",
      "         [-1.3721, -1.0078, -1.2946, -1.4787,  0.1280,  1.0649,  0.2840,\n",
      "           0.0870],\n",
      "         [-1.3671, -1.0144, -1.2945, -1.4760,  0.1437,  1.0666,  0.2824,\n",
      "           0.0944],\n",
      "         [-1.3817, -1.0133, -1.2906, -1.4793,  0.1225,  1.0539,  0.2804,\n",
      "           0.0728],\n",
      "         [-1.3808, -1.0256, -1.2878, -1.4787,  0.1351,  1.0477,  0.2740,\n",
      "           0.0705],\n",
      "         [-1.3691, -1.0370, -1.2861, -1.4915,  0.1297,  1.0409,  0.2583,\n",
      "           0.0617]]], grad_fn=<MulBackward0>), tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000],\n",
      "          [ 0.5403,  0.9950,  0.9999,  1.0000,  0.5403,  0.9950,  0.9999,\n",
      "            1.0000],\n",
      "          [-0.4161,  0.9801,  0.9998,  1.0000, -0.4161,  0.9801,  0.9998,\n",
      "            1.0000],\n",
      "          [-0.9900,  0.9553,  0.9996,  1.0000, -0.9900,  0.9553,  0.9996,\n",
      "            1.0000],\n",
      "          [-0.6536,  0.9211,  0.9992,  1.0000, -0.6536,  0.9211,  0.9992,\n",
      "            1.0000],\n",
      "          [ 0.2837,  0.8776,  0.9988,  1.0000,  0.2837,  0.8776,  0.9988,\n",
      "            1.0000],\n",
      "          [ 0.9602,  0.8253,  0.9982,  1.0000,  0.9602,  0.8253,  0.9982,\n",
      "            1.0000]]]]), tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000],\n",
      "          [ 0.8415,  0.0998,  0.0100,  0.0010,  0.8415,  0.0998,  0.0100,\n",
      "            0.0010],\n",
      "          [ 0.9093,  0.1987,  0.0200,  0.0020,  0.9093,  0.1987,  0.0200,\n",
      "            0.0020],\n",
      "          [ 0.1411,  0.2955,  0.0300,  0.0030,  0.1411,  0.2955,  0.0300,\n",
      "            0.0030],\n",
      "          [-0.7568,  0.3894,  0.0400,  0.0040, -0.7568,  0.3894,  0.0400,\n",
      "            0.0040],\n",
      "          [-0.9589,  0.4794,  0.0500,  0.0050, -0.9589,  0.4794,  0.0500,\n",
      "            0.0050],\n",
      "          [-0.2794,  0.5646,  0.0600,  0.0060, -0.2794,  0.5646,  0.0600,\n",
      "            0.0060]]]]))\n"
     ]
    }
   ],
   "source": [
    "model = LagLlamaModelNorm1(input_size=3, embedding_dim=8)  # Misalnya batch size 1, sequence length 7, feature size 3\n",
    "output_rms_1 = model(lag_features_new)\n",
    "print(\"Output after RMSNorm:\", output_rms_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CausalSelfAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., :x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, pos_emb):\n",
    "    cos, sin = pos_emb[..., :q.shape[-1] // 2], pos_emb[..., q.shape[-1] // 2:]\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == self.embed_size, \"Embed size needs to be divisible by heads\"\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.rotary_embedding = LlamaRotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, value_len, key_len, query_len = values.shape[0], values.shape[1], keys.shape[1], query.shape[1]\n",
    "        print(\"Initial shapes - Queries:\", query.shape, \"Keys:\", keys.shape, \"Values:\", values.shape)\n",
    "        \n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # Apply rotary positional embedding to queries and keys\n",
    "        pos_emb = self.rotary_embedding(query, query_len)\n",
    "        queries, keys = apply_rotary_pos_emb(queries, keys, pos_emb)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagLlamaModelCaus(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, heads):\n",
    "        super(LagLlamaModelCaus, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, embedding_dim)  # Layer linear untuk embedding\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rotary_embedding = LlamaRotaryEmbedding(embedding_dim)\n",
    "        self.rms_norm = RMSNorm(embedding_dim)\n",
    "        self.causal_attention = CausalSelfAttention(embedding_dim, heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Aplikasi embedding linear\n",
    "        x = self.rms_norm(x)\n",
    "        cos, sin = self.rotary_embedding(x.device, x.dtype, x.shape[1])\n",
    "        x = self.causal_attention(x,x,x,None)\n",
    "        return x, cos, sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 8\n",
    "model = LagLlamaModelCaus(input_size, embedding_dim, heads)\n",
    "output = model(lag_features_new)\n",
    "print(\"Output after CausalSelfAttention:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
