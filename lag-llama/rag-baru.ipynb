{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abizard/miniconda3/envs/finetuning/lib/python3.12/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
    "import yfinance as yf\n",
    "from ts_rag import TimeSeriesRAG\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "import faiss\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00369009 0.48747525 0.36218342 0.28627118 0.4686708  0.47406274\n",
      "  0.33767748 0.15505007]\n",
      " [0.08535599 0.6126571  0.9311839  0.0643617  0.21316479 0.6362434\n",
      "  0.37396628 0.12177262]\n",
      " [0.15464704 0.00990624 0.33698612 0.41654465 0.14822309 0.53762555\n",
      "  0.40876317 0.7903428 ]\n",
      " [0.40824002 0.07783359 0.03861525 0.57236826 0.7004606  0.8714922\n",
      "  0.89963615 0.11670821]\n",
      " [0.6481953  0.45665488 0.9394476  0.93626153 0.579651   0.06387179\n",
      "  0.27538157 0.7074047 ]\n",
      " [0.48537177 0.6829692  0.6686738  0.6175575  0.5389405  0.70927894\n",
      "  0.7924568  0.29609832]\n",
      " [0.4092778  0.23745726 0.26349208 0.6909923  0.27169982 0.99399626\n",
      "  0.20828663 0.9134733 ]\n",
      " [0.77403045 0.43002063 0.9089085  0.18681653 0.3867646  0.18769099\n",
      "  0.03825889 0.8326541 ]\n",
      " [0.0308917  0.07205846 0.8901135  0.3127538  0.59862953 0.30868712\n",
      "  0.31308222 0.40041345]\n",
      " [0.6990329  0.3040496  0.35295743 0.7534351  0.62928313 0.9106741\n",
      "  0.32662997 0.08992064]\n",
      " [0.6108108  0.076092   0.01940372 0.80075675 0.7271038  0.29510275\n",
      "  0.3258918  0.4488515 ]\n",
      " [0.06650311 0.6742237  0.7522964  0.7639429  0.686436   0.43512395\n",
      "  0.01273567 0.9009335 ]\n",
      " [0.8335009  0.5745554  0.08550814 0.10332154 0.19209795 0.31887513\n",
      "  0.92946386 0.2553566 ]\n",
      " [0.05455356 0.7408778  0.7545793  0.36594698 0.3850182  0.88064176\n",
      "  0.28047258 0.43496546]\n",
      " [0.17455345 0.3766609  0.50339204 0.09768684 0.5763707  0.5033\n",
      "  0.49407193 0.8595793 ]\n",
      " [0.3783234  0.5761912  0.7250027  0.32897297 0.00757611 0.76382667\n",
      "  0.98155916 0.4672124 ]\n",
      " [0.8636055  0.22302705 0.43098357 0.18017147 0.24731006 0.40744045\n",
      "  0.28038782 0.46192205]\n",
      " [0.5520873  0.00446449 0.34867844 0.95386934 0.76687914 0.14929572\n",
      "  0.50374585 0.3486968 ]\n",
      " [0.30843598 0.8992012  0.8428615  0.41477704 0.13368315 0.28660917\n",
      "  0.51964444 0.88431495]\n",
      " [0.5516642  0.7785105  0.04949335 0.18258065 0.6162155  0.9216208\n",
      "  0.19833055 0.3414761 ]]\n",
      "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f4738f181e0> >\n",
      "Jarak: [[0.38277695 0.63994193 0.7390079  0.9115759  0.9265491 ]]\n",
      "Indeks: [[14 15 18 13  5]]\n",
      "[[0.17455345 0.3766609  0.50339204 0.09768684 0.5763707  0.5033\n",
      "  0.49407193 0.8595793 ]\n",
      " [0.3783234  0.5761912  0.7250027  0.32897297 0.00757611 0.76382667\n",
      "  0.98155916 0.4672124 ]\n",
      " [0.30843598 0.8992012  0.8428615  0.41477704 0.13368315 0.28660917\n",
      "  0.51964444 0.88431495]\n",
      " [0.05455356 0.7408778  0.7545793  0.36594698 0.3850182  0.88064176\n",
      "  0.28047258 0.43496546]\n",
      " [0.48537177 0.6829692  0.6686738  0.6175575  0.5389405  0.70927894\n",
      "  0.7924568  0.29609832]]\n"
     ]
    }
   ],
   "source": [
    "time_series_data = np.random.rand(20, 8).astype('float32')\n",
    "print(time_series_data)\n",
    "\n",
    "index = faiss.IndexFlatL2(8)\n",
    "print(index)\n",
    "index.add(time_series_data)\n",
    "\n",
    "query = np.random.rand(1, 8).astype('float32') \n",
    "D, I = index.search(query, 5)\n",
    "\n",
    "print('Jarak:', D)\n",
    "print('Indeks:', I)\n",
    "\n",
    "similar_time_series = time_series_data[I[0]]\n",
    "print(similar_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stock_data(symbol: list, start_date: str) -> pd.DataFrame:\n",
    "    df = (\n",
    "        yf.Tickers(symbol)\n",
    "        .history(period=\"max\", start=start_date)\n",
    "        .Close\n",
    "        .resample('1d')\n",
    "        .ffill()\n",
    "    )\n",
    "    df = df.rolling(5).mean().pct_change().dropna()\n",
    "    return df\n",
    "\n",
    "def create_gluonts_dataset(dataset):\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype != \"object\" and not pd.api.types.is_string_dtype(dataset[col]):\n",
    "            dataset[col] = dataset[col].astype(\"float32\")\n",
    "\n",
    "    backtest_dataset = PandasDataset(dict(dataset))\n",
    "    return backtest_dataset\n",
    "\n",
    "def evaluate_predictions(actual, predictions, prediction_length):\n",
    "    actual = actual[-prediction_length:]\n",
    "    mse = np.mean((actual - predictions) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(actual - predictions))\n",
    "    mape = np.mean(np.abs((actual - predictions) / actual)) * 100\n",
    "    \n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, device, context_length=32, use_rope_scaling=False, num_samples=100):\n",
    "    ckpt = torch.load(\"./lag-llama-model/lag-llama.ckpt\", map_location=device) # Uses GPU since in this Colab we use a GPU.\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    rope_scaling_arguments = {\n",
    "        \"type\": \"linear\",\n",
    "        \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "    }\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"./lag-llama-model/lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length, # Lag-Llama was trained with a context length of 32, but can work with any context length\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "        rope_scaling=rope_scaling_arguments if use_rope_scaling else None,\n",
    "\n",
    "        batch_size=1,\n",
    "        num_parallel_samples=100,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_rag_predictions(\n",
    "    dataset,\n",
    "    prediction_length,\n",
    "    device,\n",
    "    context_length=32,\n",
    "    use_rope_scaling=False,\n",
    "    num_samples=100\n",
    "):\n",
    "    \"\"\"Get Lag-Llama predictions augmented with RAG\"\"\"\n",
    "    \n",
    "    # Initialize RAG\n",
    "    rag = TimeSeriesRAG(\n",
    "        embedding_dim=128,\n",
    "        num_neighbors=5,\n",
    "        similarity_threshold=0.7\n",
    "    )\n",
    "    \n",
    "    # Get base predictions from Lag-Llama\n",
    "    base_forecasts, tss = get_lag_llama_predictions(\n",
    "        dataset,\n",
    "        prediction_length,\n",
    "        device,\n",
    "        context_length,\n",
    "        use_rope_scaling,\n",
    "        num_samples\n",
    "    )\n",
    "    print(base_forecasts)\n",
    "    print(type(base_forecasts))\n",
    "    print(type(tss))\n",
    "    \n",
    "    augmented_forecasts = []\n",
    "    \n",
    "    # Process each prediction\n",
    "    for i, (forecast, ts) in enumerate(zip(base_forecasts, tss)):\n",
    "        # Get similar sequences from RAG\n",
    "        series_values = ts[0].values\n",
    "        similar_seqs = rag.retrieve_similar(series_values)  # Retrieve similar time series based on the target series\n",
    "        print(similar_seqs)\n",
    "        # Augment prediction using RAG\n",
    "        base_samples = forecast\n",
    "        print('HALOOOO',base_samples)\n",
    "        augmented_samples = rag.augment_prediction(base_samples, similar_seqs)\n",
    "        \n",
    "        # Create new forecast object with augmented predictions\n",
    "        augmented_forecast = forecast\n",
    "        augmented_forecast.samples = augmented_samples\n",
    "        augmented_forecasts.append(augmented_forecast)\n",
    "        print('Augment', augmented_forecasts)\n",
    "        \n",
    "        # Add current prediction to index for future use\n",
    "        rag.add_to_index(\n",
    "        series_values,\n",
    "        {\n",
    "            'historical_prediction': forecast.samples,\n",
    "            'metadata': {'item_id': forecast.item_id}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return augmented_forecasts, tss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock_with_rag(symbol, start_date, prediction_length=5, context_length=20, device=\"cuda:0\"):\n",
    "    df = prepare_stock_data(symbol, start_date)\n",
    "    dataset = create_gluonts_dataset(df)\n",
    "\n",
    "    # Get predictions using Lag-Llama augmented with RAG\n",
    "    forecasts, tss = get_lag_llama_rag_predictions(dataset, prediction_length, device, context_length, num_samples=100)\n",
    "\n",
    "    mean_predictions = []\n",
    "    for forecast in forecasts:\n",
    "        mean_pred = forecast.mean\n",
    "        mean_predictions.append(mean_pred)\n",
    "\n",
    "    predictions = np.array(mean_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_predictions(df['target'].values, predictions, prediction_length)\n",
    "    print(predictions)\n",
    "    return predictions, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    }
   ],
   "source": [
    "df = prepare_stock_data(['AAPL', 'MSFT'], '2013-01-01')\n",
    "train_dataset = create_gluonts_dataset(df.iloc[:int(0.7*len(df))])\n",
    "test_dataset = create_gluonts_dataset(df.iloc[int(0.7*len(df)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 60\n",
    "context_length = 32\n",
    "device = torch.device(\"cuda:0\") \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "forecasts_rag, tss = get_lag_llama_rag_predictions(test_dataset, prediction_length, device, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_380205/42396323.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"./lag-llama-model/lag-llama.ckpt\", map_location=device) # Uses GPU since in this Colab we use a GPU.\n",
      "/home/abizard/miniconda3/envs/finetuning/lib/python3.12/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/abizard/miniconda3/envs/finetuning/lib/python3.12/site-packages/gluonts/dataset/pandas.py:174: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df = df.to_period(freq=self.freq)\n",
      "/home/abizard/miniconda3/envs/finetuning/lib/python3.12/site-packages/gluonts/dataset/pandas.py:174: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df = df.to_period(freq=self.freq)\n",
      "/home/abizard/miniconda3/envs/finetuning/lib/python3.12/site-packages/gluonts/dataset/pandas.py:174: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df = df.to_period(freq=self.freq)\n"
     ]
    }
   ],
   "source": [
    "prediction_length = 60\n",
    "context_length = 32\n",
    "device = torch.device(\"cuda:0\") \n",
    "\n",
    "forecast_it, tss_it = get_lag_llama_predictions(test_dataset, prediction_length, device, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 2 sequences\n",
      "Current index size: 2\n",
      "Query embedding shape: (32,)\n",
      "Query embedding dtype: float32\n",
      "Query embedding sample: [-0.23121846 -0.2272958  -2.9947195   2.649545   -0.23393995 -0.23103146\n",
      " -0.22871274 -0.23155153  0.26563936 -0.18134485  0.28666496  0.5742782\n",
      "  0.06282052  0.28124228  0.16755196  0.01591028  0.2561621   0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n",
      "Searching for 2 neighbors\n",
      "Raw distances: [[0.         0.33531463]]\n",
      "Raw indices: [[1 0]]\n",
      "Distance: 0.0, Similarity: 1.0\n",
      "Distance: 0.33531463146209717, Similarity: 0.7488871734334658\n",
      "[({'historical_prediction': array([[-0.00129487,  0.00127371,  0.00111405, ...,  0.00099258,\n",
      "        -0.00328832,  0.00028685],\n",
      "       [-0.00155724,  0.00595453,  0.00863928, ...,  0.00161305,\n",
      "         0.00486501,  0.00572697],\n",
      "       [-0.00522363,  0.00145362,  0.00121425, ...,  0.00443372,\n",
      "         0.00722506,  0.00820471],\n",
      "       ...,\n",
      "       [ 0.00036599,  0.00091562,  0.0031799 , ...,  0.00368625,\n",
      "        -0.00241555,  0.0008076 ],\n",
      "       [-0.00428716, -0.00375983, -0.00495914, ...,  0.00707331,\n",
      "         0.01073234,  0.00773441],\n",
      "       [-0.00326834,  0.00440682,  0.00483132, ..., -0.01861085,\n",
      "        -0.01524023, -0.01269052]], dtype=float32), 'metadata': {'item_id': 'MSFT'}}, 1.0), ({'historical_prediction': array([[-0.00432965, -0.00525395, -0.00508713, ..., -0.00384713,\n",
      "        -0.00467628, -0.00897329],\n",
      "       [-0.00019225, -0.00449323, -0.00315738, ...,  0.00142574,\n",
      "         0.00162743,  0.00171829],\n",
      "       [-0.00381974,  0.00060387, -0.00460991, ..., -0.01474637,\n",
      "        -0.01003662, -0.00900958],\n",
      "       ...,\n",
      "       [-0.00986601, -0.00881577, -0.01333725, ...,  0.00319905,\n",
      "         0.00198703,  0.01225696],\n",
      "       [-0.00670062, -0.0037541 , -0.00393631, ...,  0.00059414,\n",
      "        -0.00162784,  0.0002247 ],\n",
      "       [-0.00515335, -0.00253631, -0.0010269 , ..., -0.01322641,\n",
      "        -0.00722164, -0.00502737]], dtype=float32), 'metadata': {'item_id': 'AAPL'}}, 0.7488871734334658)]\n",
      "Retrieved sequences: 2\n"
     ]
    }
   ],
   "source": [
    "rag = TimeSeriesRAG(\n",
    "        embedding_dim=32,\n",
    "        num_neighbors=5,\n",
    "        similarity_threshold=0.7\n",
    "    )\n",
    "\n",
    "for forecast, ts in zip(forecast_it, tss_it):\n",
    "    series_values = ts[0].values\n",
    "    rag.add_to_index(\n",
    "        series_values,\n",
    "        {\n",
    "            'historical_prediction': forecast.samples,\n",
    "            'metadata': {'item_id': forecast.item_id}\n",
    "        }\n",
    "    )\n",
    "print(f\"Index built with {rag.index.ntotal} sequences\")\n",
    "\n",
    "# Coba retrieval\n",
    "similar_seqs = rag.retrieve_similar(series_values)\n",
    "print(similar_seqs)\n",
    "print(\"Retrieved sequences:\", len(similar_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_it[1].samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_it[0].samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = [\"AAPL\"]\n",
    "start_date = \"2013-01-01\"\n",
    "prediction_length = 60\n",
    "context_length = 32\n",
    "device = torch.device(\"cuda:0\") \n",
    "\n",
    "predictions, metrics = predict_stock_with_rag(symbol, start_date, prediction_length, context_length, device)\n",
    "print(f\"Prediksi untuk {symbol}:\")\n",
    "print(f\"Prediksi harga saham 5 hari ke depan: {predictions}\")\n",
    "print(\"\\nMetrik Evaluasi:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
